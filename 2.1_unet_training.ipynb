{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentatation\n",
    "ref:\n",
    "https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/how_to_use_nnunet.md\n",
    "https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/set_environment_variables.md \n",
    "\n",
    "\n",
    "env: python3.12 torch\n",
    "\n",
    "## Example dataset.json:\n",
    "```json\n",
    "{\n",
    "    \"channel_names\": {\n",
    "        \"0\": \"CT\"\n",
    "    },\n",
    "    \"labels\": {\n",
    "        \"background\": 0,\n",
    "        \"blood\": 1,\n",
    "        \"aaa\": 2\n",
    "    },\n",
    "    \"numTraining\": 50,\n",
    "    \"file_ending\": \".nii.gz\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: nnUNet_raw=data/nnunet_raw\n",
      "env: nnUNet_preprocessed=data/nnunet_preprocessed\n",
      "env: nnUNet_results=data/nnunet_results\n",
      "Fingerprint extraction...\n",
      "Dataset002_perlin_aaa\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "\n",
      "####################\n",
      "verify_dataset_integrity Done. \n",
      "If you didn't see any error messages then your dataset is most likely OK!\n",
      "####################\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  9.39it/s]\n",
      "Experiment planning...\n",
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default planner. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Dropping 3d_lowres config because the image size difference to 3d_fullres is too small. 3d_fullres: [213. 130. 149.], 3d_lowres: [213, 130, 149]\n",
      "2D U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 127, 'patch_size': (np.int64(160), np.int64(160)), 'median_image_size_in_voxels': array([130., 149.]), 'spacing': array([1., 1.]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 512, 512), 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': ((3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)), 'strides': ((1, 1), (2, 2), (2, 2), (2, 2), (2, 2), (2, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': True}\n",
      "\n",
      "Using <class 'nnunetv2.imageio.simpleitk_reader_writer.SimpleITKIO'> as reader/writer\n",
      "3D fullres U-Net configuration:\n",
      "{'data_identifier': 'nnUNetPlans_3d_fullres', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 2, 'patch_size': (np.int64(160), np.int64(112), np.int64(128)), 'median_image_size_in_voxels': array([213., 130., 149.]), 'spacing': array([1., 1., 1.]), 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': (32, 64, 128, 256, 320, 320), 'conv_op': 'torch.nn.modules.conv.Conv3d', 'kernel_sizes': ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)), 'strides': ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 1, 2)), 'n_conv_per_stage': (2, 2, 2, 2, 2, 2), 'n_conv_per_stage_decoder': (2, 2, 2, 2, 2), 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm3d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}}, '_kw_requires_import': ('conv_op', 'norm_op', 'dropout_op', 'nonlin')}, 'batch_dice': False}\n",
      "\n",
      "Plans were saved to data/nnunet_preprocessed/Dataset002_perlin_aaa/nnUNetPlans.json\n",
      "Preprocessing...\n",
      "Preprocessing dataset Dataset002_perlin_aaa\n",
      "Configuration: 2d...\n",
      "100%|███████████████████████████████████████████| 50/50 [00:21<00:00,  2.29it/s]\n",
      "Configuration: 3d_fullres...\n",
      "100%|███████████████████████████████████████████| 50/50 [00:31<00:00,  1.61it/s]\n",
      "Configuration: 3d_lowres...\n",
      "INFO: Configuration 3d_lowres not found in plans file nnUNetPlans.json of dataset Dataset002_perlin_aaa. Skipping.\n"
     ]
    }
   ],
   "source": [
    "%env nnUNet_raw=data/nnunet_raw\n",
    "%env nnUNet_preprocessed=data/nnunet_preprocessed\n",
    "%env nnUNet_results=data/nnunet_results\n",
    "\n",
    "!nnUNetv2_plan_and_preprocess -d 2 --verify_dataset_integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################\n",
      "INFO: You are using the old nnU-Net default plans. We have updated our recommendations. Please consider using those instead! Read more here: https://github.com/MIC-DKFZ/nnUNet/blob/master/documentation/resenc_presets.md\n",
      "############################\n",
      "\n",
      "Using device: cuda:0\n",
      "/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py:164: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.grad_scaler = GradScaler() if self.device.type == 'cuda' else None\n",
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "2024-10-02 20:12:43.024613: do_dummy_2d_data_aug: False\n",
      "using pin_memory on device 0\n",
      "using pin_memory on device 0\n",
      "2024-10-02 20:12:47.330701: Using torch.compile...\n",
      "/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "\n",
      "This is the configuration used by this training:\n",
      "Configuration name: 2d\n",
      " {'data_identifier': 'nnUNetPlans_2d', 'preprocessor_name': 'DefaultPreprocessor', 'batch_size': 198, 'patch_size': [128, 128], 'median_image_size_in_voxels': [123.0, 120.0], 'spacing': [1.0, 1.0], 'normalization_schemes': ['CTNormalization'], 'use_mask_for_norm': [False], 'resampling_fn_data': 'resample_data_or_seg_to_shape', 'resampling_fn_seg': 'resample_data_or_seg_to_shape', 'resampling_fn_data_kwargs': {'is_seg': False, 'order': 3, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_seg_kwargs': {'is_seg': True, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'resampling_fn_probabilities': 'resample_data_or_seg_to_shape', 'resampling_fn_probabilities_kwargs': {'is_seg': False, 'order': 1, 'order_z': 0, 'force_separate_z': None}, 'architecture': {'network_class_name': 'dynamic_network_architectures.architectures.unet.PlainConvUNet', 'arch_kwargs': {'n_stages': 6, 'features_per_stage': [32, 64, 128, 256, 512, 512], 'conv_op': 'torch.nn.modules.conv.Conv2d', 'kernel_sizes': [[3, 3], [3, 3], [3, 3], [3, 3], [3, 3], [3, 3]], 'strides': [[1, 1], [2, 2], [2, 2], [2, 2], [2, 2], [2, 2]], 'n_conv_per_stage': [2, 2, 2, 2, 2, 2], 'n_conv_per_stage_decoder': [2, 2, 2, 2, 2], 'conv_bias': True, 'norm_op': 'torch.nn.modules.instancenorm.InstanceNorm2d', 'norm_op_kwargs': {'eps': 1e-05, 'affine': True}, 'dropout_op': None, 'dropout_op_kwargs': None, 'nonlin': 'torch.nn.LeakyReLU', 'nonlin_kwargs': {'inplace': True}, 'deep_supervision': True}, '_kw_requires_import': ['conv_op', 'norm_op', 'dropout_op', 'nonlin']}, 'batch_dice': True} \n",
      "\n",
      "These are the global plan.json settings:\n",
      " {'dataset_name': 'Dataset001_aaa', 'plans_name': 'nnUNetPlans', 'original_median_spacing_after_transp': [1.0, 1.0, 1.0], 'original_median_shape_after_transp': [231, 123, 120], 'image_reader_writer': 'SimpleITKIO', 'transpose_forward': [0, 1, 2], 'transpose_backward': [0, 1, 2], 'experiment_planner_used': 'ExperimentPlanner', 'label_manager': 'LabelManager', 'foreground_intensity_properties_per_channel': {'0': {'max': 1.0000001192092896, 'mean': 0.42032840847969055, 'median': 0.40833938121795654, 'min': 0.0003197960904799402, 'percentile_00_5': 0.002681480720639229, 'percentile_99_5': 0.9963263869285583, 'std': 0.26408493518829346}}} \n",
      "\n",
      "2024-10-02 20:12:48.293705: unpacking dataset...\n",
      "2024-10-02 20:12:52.316682: unpacking done...\n",
      "2024-10-02 20:12:52.317478: Unable to plot network architecture: nnUNet_compile is enabled!\n",
      "2024-10-02 20:12:52.333686: \n",
      "2024-10-02 20:12:52.333785: Epoch 0\n",
      "2024-10-02 20:12:52.333910: Current learning rate: 0.01\n",
      "W1002 20:13:03.422000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:03.523000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:03.609000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:03.691000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:03.776000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:15.447000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:15.491000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:15.824000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:15.864000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:16.101000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:16.142000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:16.390000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:16.430000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:17.284000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:17.373000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:17.934000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:18.172000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:18.358000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:18.779000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:13:19.468000 140090181412416 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:15:39.729000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:15:39.877000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:15:40.020000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:15:40.155000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "W1002 20:15:40.286000 140095234696256 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] xindex is not in var_ranges, defaulting to unknown range.\n",
      "2024-10-02 20:15:52.964347: train_loss 0.0005\n",
      "2024-10-02 20:15:52.964617: val_loss -0.3543\n",
      "2024-10-02 20:15:52.964770: Pseudo dice [0.6086, 0.6897]\n",
      "2024-10-02 20:15:52.964912: Epoch time: 180.63 s\n",
      "2024-10-02 20:15:52.964993: Yayy! New best EMA pseudo Dice: 0.6491\n",
      "2024-10-02 20:15:54.114862: \n",
      "2024-10-02 20:15:54.115216: Epoch 1\n",
      "2024-10-02 20:15:54.115355: Current learning rate: 0.00999\n",
      "2024-10-02 20:18:28.205753: train_loss -0.5227\n",
      "2024-10-02 20:18:28.206242: val_loss -0.7476\n",
      "2024-10-02 20:18:28.206332: Pseudo dice [0.8359, 0.895]\n",
      "2024-10-02 20:18:28.206417: Epoch time: 154.09 s\n",
      "2024-10-02 20:18:28.206472: Yayy! New best EMA pseudo Dice: 0.6708\n",
      "2024-10-02 20:18:29.408317: \n",
      "2024-10-02 20:18:29.408490: Epoch 2\n",
      "2024-10-02 20:18:29.408592: Current learning rate: 0.00998\n",
      "2024-10-02 20:20:51.077552: train_loss -0.7706\n",
      "2024-10-02 20:20:51.078131: val_loss -0.8485\n",
      "2024-10-02 20:20:51.078449: Pseudo dice [0.8998, 0.9379]\n",
      "2024-10-02 20:20:51.078665: Epoch time: 141.67 s\n",
      "2024-10-02 20:20:51.078851: Yayy! New best EMA pseudo Dice: 0.6956\n",
      "2024-10-02 20:20:52.363594: \n",
      "2024-10-02 20:20:52.363730: Epoch 3\n",
      "2024-10-02 20:20:52.363842: Current learning rate: 0.00997\n",
      "2024-10-02 20:23:22.491565: train_loss -0.8384\n",
      "2024-10-02 20:23:22.491762: val_loss -0.8773\n",
      "2024-10-02 20:23:22.491845: Pseudo dice [0.9164, 0.9495]\n",
      "2024-10-02 20:23:22.491935: Epoch time: 150.13 s\n",
      "2024-10-02 20:23:22.492078: Yayy! New best EMA pseudo Dice: 0.7193\n",
      "2024-10-02 20:23:23.714383: \n",
      "2024-10-02 20:23:23.714724: Epoch 4\n",
      "2024-10-02 20:23:23.714842: Current learning rate: 0.00996\n",
      "2024-10-02 20:25:40.826271: train_loss -0.8638\n",
      "2024-10-02 20:25:40.826460: val_loss -0.8918\n",
      "2024-10-02 20:25:40.826548: Pseudo dice [0.926, 0.9549]\n",
      "2024-10-02 20:25:40.826622: Epoch time: 137.11 s\n",
      "2024-10-02 20:25:40.826675: Yayy! New best EMA pseudo Dice: 0.7414\n",
      "2024-10-02 20:25:42.146536: \n",
      "2024-10-02 20:25:42.146682: Epoch 5\n",
      "2024-10-02 20:25:42.146787: Current learning rate: 0.00995\n",
      "2024-10-02 20:28:00.212829: train_loss -0.8758\n",
      "2024-10-02 20:28:00.213364: val_loss -0.8986\n",
      "2024-10-02 20:28:00.213494: Pseudo dice [0.9295, 0.9587]\n",
      "2024-10-02 20:28:00.213608: Epoch time: 138.07 s\n",
      "2024-10-02 20:28:00.213683: Yayy! New best EMA pseudo Dice: 0.7617\n",
      "2024-10-02 20:28:01.435670: \n",
      "2024-10-02 20:28:01.436194: Epoch 6\n",
      "2024-10-02 20:28:01.436297: Current learning rate: 0.00995\n",
      "2024-10-02 20:30:18.611649: train_loss -0.8858\n",
      "2024-10-02 20:30:18.611830: val_loss -0.9027\n",
      "2024-10-02 20:30:18.611901: Pseudo dice [0.9317, 0.9612]\n",
      "2024-10-02 20:30:18.611962: Epoch time: 137.18 s\n",
      "2024-10-02 20:30:18.612005: Yayy! New best EMA pseudo Dice: 0.7802\n",
      "2024-10-02 20:30:19.986070: \n",
      "2024-10-02 20:30:19.986484: Epoch 7\n",
      "2024-10-02 20:30:19.986687: Current learning rate: 0.00994\n",
      "2024-10-02 20:32:30.414982: train_loss -0.8906\n",
      "2024-10-02 20:32:30.415249: val_loss -0.9072\n",
      "2024-10-02 20:32:30.415346: Pseudo dice [0.9354, 0.9621]\n",
      "2024-10-02 20:32:30.415412: Epoch time: 130.43 s\n",
      "2024-10-02 20:32:30.415477: Yayy! New best EMA pseudo Dice: 0.797\n",
      "2024-10-02 20:32:31.682446: \n",
      "2024-10-02 20:32:31.682778: Epoch 8\n",
      "2024-10-02 20:32:31.682976: Current learning rate: 0.00993\n",
      "2024-10-02 20:34:39.742948: train_loss -0.894\n",
      "2024-10-02 20:34:39.743122: val_loss -0.9118\n",
      "2024-10-02 20:34:39.743193: Pseudo dice [0.9384, 0.9645]\n",
      "2024-10-02 20:34:39.743264: Epoch time: 128.06 s\n",
      "2024-10-02 20:34:39.743313: Yayy! New best EMA pseudo Dice: 0.8125\n",
      "2024-10-02 20:34:41.064690: \n",
      "2024-10-02 20:34:41.064874: Epoch 9\n",
      "2024-10-02 20:34:41.065009: Current learning rate: 0.00992\n",
      "2024-10-02 20:36:47.011183: train_loss -0.8981\n",
      "2024-10-02 20:36:47.011425: val_loss -0.9124\n",
      "2024-10-02 20:36:47.011614: Pseudo dice [0.9388, 0.9643]\n",
      "2024-10-02 20:36:47.011805: Epoch time: 125.95 s\n",
      "2024-10-02 20:36:47.012171: Yayy! New best EMA pseudo Dice: 0.8264\n",
      "2024-10-02 20:36:48.223668: \n",
      "2024-10-02 20:36:48.223803: Epoch 10\n",
      "2024-10-02 20:36:48.223905: Current learning rate: 0.00991\n",
      "2024-10-02 20:39:04.014933: train_loss -0.9008\n",
      "2024-10-02 20:39:04.015099: val_loss -0.915\n",
      "2024-10-02 20:39:04.015215: Pseudo dice [0.9403, 0.9658]\n",
      "2024-10-02 20:39:04.015375: Epoch time: 135.79 s\n",
      "2024-10-02 20:39:04.015560: Yayy! New best EMA pseudo Dice: 0.839\n",
      "2024-10-02 20:39:05.329397: \n",
      "2024-10-02 20:39:05.329684: Epoch 11\n",
      "2024-10-02 20:39:05.329805: Current learning rate: 0.0099\n",
      "2024-10-02 20:41:12.745313: train_loss -0.9038\n",
      "2024-10-02 20:41:12.745695: val_loss -0.9178\n",
      "2024-10-02 20:41:12.745809: Pseudo dice [0.9416, 0.9671]\n",
      "2024-10-02 20:41:12.746029: Epoch time: 127.42 s\n",
      "2024-10-02 20:41:12.746103: Yayy! New best EMA pseudo Dice: 0.8506\n",
      "2024-10-02 20:41:13.981093: \n",
      "2024-10-02 20:41:13.981553: Epoch 12\n",
      "2024-10-02 20:41:13.981690: Current learning rate: 0.00989\n",
      "2024-10-02 20:43:22.205777: train_loss -0.9057\n",
      "2024-10-02 20:43:22.206238: val_loss -0.9202\n",
      "2024-10-02 20:43:22.206424: Pseudo dice [0.944, 0.9674]\n",
      "2024-10-02 20:43:22.206580: Epoch time: 128.23 s\n",
      "2024-10-02 20:43:22.206679: Yayy! New best EMA pseudo Dice: 0.8611\n",
      "2024-10-02 20:43:23.551742: \n",
      "2024-10-02 20:43:23.552037: Epoch 13\n",
      "2024-10-02 20:43:23.552772: Current learning rate: 0.00988\n",
      "2024-10-02 20:45:35.258578: train_loss -0.9076\n",
      "2024-10-02 20:45:35.258746: val_loss -0.9201\n",
      "2024-10-02 20:45:35.258852: Pseudo dice [0.9433, 0.9679]\n",
      "2024-10-02 20:45:35.258947: Epoch time: 131.71 s\n",
      "2024-10-02 20:45:35.259015: Yayy! New best EMA pseudo Dice: 0.8705\n",
      "2024-10-02 20:45:36.678115: \n",
      "2024-10-02 20:45:36.678304: Epoch 14\n",
      "2024-10-02 20:45:36.678453: Current learning rate: 0.00987\n",
      "2024-10-02 20:48:01.889921: train_loss -0.9092\n",
      "2024-10-02 20:48:01.890127: val_loss -0.9219\n",
      "2024-10-02 20:48:01.890208: Pseudo dice [0.9446, 0.969]\n",
      "2024-10-02 20:48:01.890285: Epoch time: 145.21 s\n",
      "2024-10-02 20:48:01.890335: Yayy! New best EMA pseudo Dice: 0.8792\n",
      "2024-10-02 20:48:03.177698: \n",
      "2024-10-02 20:48:03.177854: Epoch 15\n",
      "2024-10-02 20:48:03.177970: Current learning rate: 0.00986\n",
      "2024-10-02 20:50:27.950914: train_loss -0.9098\n",
      "2024-10-02 20:50:27.951378: val_loss -0.9222\n",
      "2024-10-02 20:50:27.951569: Pseudo dice [0.9444, 0.9694]\n",
      "2024-10-02 20:50:27.951686: Epoch time: 144.77 s\n",
      "2024-10-02 20:50:27.951740: Yayy! New best EMA pseudo Dice: 0.8869\n",
      "2024-10-02 20:50:29.229036: \n",
      "2024-10-02 20:50:29.229196: Epoch 16\n",
      "2024-10-02 20:50:29.229321: Current learning rate: 0.00986\n",
      "2024-10-02 20:52:52.971010: train_loss -0.9113\n",
      "2024-10-02 20:52:52.971297: val_loss -0.9236\n",
      "2024-10-02 20:52:52.971375: Pseudo dice [0.9457, 0.9698]\n",
      "2024-10-02 20:52:52.971451: Epoch time: 143.74 s\n",
      "2024-10-02 20:52:52.971506: Yayy! New best EMA pseudo Dice: 0.894\n",
      "2024-10-02 20:52:54.180934: \n",
      "2024-10-02 20:52:54.181363: Epoch 17\n",
      "2024-10-02 20:52:54.181525: Current learning rate: 0.00985\n",
      "2024-10-02 20:55:19.885754: train_loss -0.9132\n",
      "2024-10-02 20:55:19.885952: val_loss -0.9263\n",
      "2024-10-02 20:55:19.886074: Pseudo dice [0.9479, 0.9707]\n",
      "2024-10-02 20:55:19.886194: Epoch time: 145.71 s\n",
      "2024-10-02 20:55:19.886282: Yayy! New best EMA pseudo Dice: 0.9005\n",
      "2024-10-02 20:55:22.442205: \n",
      "2024-10-02 20:55:22.442367: Epoch 18\n",
      "2024-10-02 20:55:22.442495: Current learning rate: 0.00984\n",
      "2024-10-02 20:57:39.852885: train_loss -0.9143\n",
      "2024-10-02 20:57:39.853197: val_loss -0.9259\n",
      "2024-10-02 20:57:39.853281: Pseudo dice [0.9477, 0.9706]\n",
      "2024-10-02 20:57:39.853368: Epoch time: 137.41 s\n",
      "2024-10-02 20:57:39.853473: Yayy! New best EMA pseudo Dice: 0.9064\n",
      "2024-10-02 20:57:41.183243: \n",
      "2024-10-02 20:57:41.183734: Epoch 19\n",
      "2024-10-02 20:57:41.183883: Current learning rate: 0.00983\n",
      "2024-10-02 21:00:11.005283: train_loss -0.9152\n",
      "2024-10-02 21:00:11.005459: val_loss -0.9277\n",
      "2024-10-02 21:00:11.005536: Pseudo dice [0.9491, 0.9713]\n",
      "2024-10-02 21:00:11.005609: Epoch time: 149.82 s\n",
      "2024-10-02 21:00:11.005666: Yayy! New best EMA pseudo Dice: 0.9118\n",
      "2024-10-02 21:00:12.262636: \n",
      "2024-10-02 21:00:12.262763: Epoch 20\n",
      "2024-10-02 21:00:12.262870: Current learning rate: 0.00982\n",
      "2024-10-02 21:02:37.279366: train_loss -0.9163\n",
      "2024-10-02 21:02:37.279551: val_loss -0.9284\n",
      "2024-10-02 21:02:37.279625: Pseudo dice [0.9495, 0.9713]\n",
      "2024-10-02 21:02:37.279718: Epoch time: 145.02 s\n",
      "2024-10-02 21:02:37.280132: Yayy! New best EMA pseudo Dice: 0.9166\n",
      "2024-10-02 21:02:38.626451: \n",
      "2024-10-02 21:02:38.626729: Epoch 21\n",
      "2024-10-02 21:02:38.626914: Current learning rate: 0.00981\n",
      "2024-10-02 21:04:47.955148: train_loss -0.917\n",
      "2024-10-02 21:04:47.955350: val_loss -0.9293\n",
      "2024-10-02 21:04:47.955424: Pseudo dice [0.9501, 0.9721]\n",
      "2024-10-02 21:04:47.955487: Epoch time: 129.33 s\n",
      "2024-10-02 21:04:47.955534: Yayy! New best EMA pseudo Dice: 0.9211\n",
      "2024-10-02 21:04:49.163496: \n",
      "2024-10-02 21:04:49.164036: Epoch 22\n",
      "2024-10-02 21:04:49.164259: Current learning rate: 0.0098\n",
      "2024-10-02 21:06:54.412078: train_loss -0.9178\n",
      "2024-10-02 21:06:54.412338: val_loss -0.9296\n",
      "2024-10-02 21:06:54.412465: Pseudo dice [0.9504, 0.9723]\n",
      "2024-10-02 21:06:54.412551: Epoch time: 125.25 s\n",
      "2024-10-02 21:06:54.412616: Yayy! New best EMA pseudo Dice: 0.9251\n",
      "2024-10-02 21:06:55.609734: \n",
      "2024-10-02 21:06:55.610144: Epoch 23\n",
      "2024-10-02 21:06:55.610255: Current learning rate: 0.00979\n",
      "2024-10-02 21:09:16.147531: train_loss -0.9189\n",
      "2024-10-02 21:09:16.148009: val_loss -0.9305\n",
      "2024-10-02 21:09:16.148135: Pseudo dice [0.9506, 0.9725]\n",
      "2024-10-02 21:09:16.148206: Epoch time: 140.54 s\n",
      "2024-10-02 21:09:16.148254: Yayy! New best EMA pseudo Dice: 0.9288\n",
      "2024-10-02 21:09:17.332290: \n",
      "2024-10-02 21:09:17.333265: Epoch 24\n",
      "2024-10-02 21:09:17.333802: Current learning rate: 0.00978\n",
      "2024-10-02 21:12:06.082900: train_loss -0.9194\n",
      "2024-10-02 21:12:06.083226: val_loss -0.9313\n",
      "2024-10-02 21:12:06.083383: Pseudo dice [0.9515, 0.9727]\n",
      "2024-10-02 21:12:06.083497: Epoch time: 168.75 s\n",
      "2024-10-02 21:12:06.083552: Yayy! New best EMA pseudo Dice: 0.9321\n",
      "2024-10-02 21:12:07.307657: \n",
      "2024-10-02 21:12:07.307788: Epoch 25\n",
      "2024-10-02 21:12:07.307898: Current learning rate: 0.00977\n",
      "2024-10-02 21:14:36.189391: train_loss -0.9205\n",
      "2024-10-02 21:14:36.189728: val_loss -0.9317\n",
      "2024-10-02 21:14:36.189948: Pseudo dice [0.9516, 0.9732]\n",
      "2024-10-02 21:14:36.190165: Epoch time: 148.88 s\n",
      "2024-10-02 21:14:36.190255: Yayy! New best EMA pseudo Dice: 0.9351\n",
      "2024-10-02 21:14:37.415453: \n",
      "2024-10-02 21:14:37.415579: Epoch 26\n",
      "2024-10-02 21:14:37.415688: Current learning rate: 0.00977\n",
      "2024-10-02 21:17:02.899188: train_loss -0.9211\n",
      "2024-10-02 21:17:02.899656: val_loss -0.9323\n",
      "2024-10-02 21:17:02.899775: Pseudo dice [0.9519, 0.973]\n",
      "2024-10-02 21:17:02.899862: Epoch time: 145.48 s\n",
      "2024-10-02 21:17:02.899952: Yayy! New best EMA pseudo Dice: 0.9379\n",
      "2024-10-02 21:17:04.136547: \n",
      "2024-10-02 21:17:04.136748: Epoch 27\n",
      "2024-10-02 21:17:04.136874: Current learning rate: 0.00976\n",
      "2024-10-02 21:19:57.943100: train_loss -0.9213\n",
      "2024-10-02 21:19:57.943364: val_loss -0.9331\n",
      "2024-10-02 21:19:57.943447: Pseudo dice [0.9524, 0.9738]\n",
      "2024-10-02 21:19:57.943522: Epoch time: 173.81 s\n",
      "2024-10-02 21:19:57.943650: Yayy! New best EMA pseudo Dice: 0.9404\n",
      "2024-10-02 21:19:59.178119: \n",
      "2024-10-02 21:19:59.178508: Epoch 28\n",
      "2024-10-02 21:19:59.178643: Current learning rate: 0.00975\n",
      "2024-10-02 21:22:49.137439: train_loss -0.9224\n",
      "2024-10-02 21:22:49.137809: val_loss -0.9334\n",
      "2024-10-02 21:22:49.138015: Pseudo dice [0.9528, 0.9739]\n",
      "2024-10-02 21:22:49.138203: Epoch time: 169.96 s\n",
      "2024-10-02 21:22:49.138322: Yayy! New best EMA pseudo Dice: 0.9427\n",
      "2024-10-02 21:22:50.452048: \n",
      "2024-10-02 21:22:50.452207: Epoch 29\n",
      "2024-10-02 21:22:50.452363: Current learning rate: 0.00974\n",
      "2024-10-02 21:25:05.679974: train_loss -0.9234\n",
      "2024-10-02 21:25:05.680176: val_loss -0.934\n",
      "2024-10-02 21:25:05.680241: Pseudo dice [0.9528, 0.974]\n",
      "2024-10-02 21:25:05.680319: Epoch time: 135.23 s\n",
      "2024-10-02 21:25:05.680365: Yayy! New best EMA pseudo Dice: 0.9448\n",
      "2024-10-02 21:25:07.007656: \n",
      "2024-10-02 21:25:07.007787: Epoch 30\n",
      "2024-10-02 21:25:07.007901: Current learning rate: 0.00973\n",
      "2024-10-02 21:27:31.655479: train_loss -0.9236\n",
      "2024-10-02 21:27:31.655645: val_loss -0.9354\n",
      "2024-10-02 21:27:31.655712: Pseudo dice [0.954, 0.9746]\n",
      "2024-10-02 21:27:31.655820: Epoch time: 144.65 s\n",
      "2024-10-02 21:27:31.655929: Yayy! New best EMA pseudo Dice: 0.9467\n",
      "2024-10-02 21:27:32.998233: \n",
      "2024-10-02 21:27:32.998358: Epoch 31\n",
      "2024-10-02 21:27:32.998469: Current learning rate: 0.00972\n",
      "2024-10-02 21:29:51.233206: train_loss -0.9236\n",
      "2024-10-02 21:29:51.233397: val_loss -0.9352\n",
      "2024-10-02 21:29:51.233580: Pseudo dice [0.9543, 0.9743]\n",
      "2024-10-02 21:29:51.233672: Epoch time: 138.24 s\n",
      "2024-10-02 21:29:51.233709: Yayy! New best EMA pseudo Dice: 0.9485\n",
      "2024-10-02 21:29:52.644121: \n",
      "2024-10-02 21:29:52.644253: Epoch 32\n",
      "2024-10-02 21:29:52.644481: Current learning rate: 0.00971\n",
      "2024-10-02 21:32:05.801999: train_loss -0.9247\n",
      "2024-10-02 21:32:05.802213: val_loss -0.9347\n",
      "2024-10-02 21:32:05.802375: Pseudo dice [0.9532, 0.9746]\n",
      "2024-10-02 21:32:05.802473: Epoch time: 133.16 s\n",
      "2024-10-02 21:32:05.802579: Yayy! New best EMA pseudo Dice: 0.95\n",
      "2024-10-02 21:32:07.166978: \n",
      "2024-10-02 21:32:07.167154: Epoch 33\n",
      "2024-10-02 21:32:07.167271: Current learning rate: 0.0097\n",
      "2024-10-02 21:34:18.812485: train_loss -0.925\n",
      "2024-10-02 21:34:18.813047: val_loss -0.9357\n",
      "2024-10-02 21:34:18.813234: Pseudo dice [0.9542, 0.9749]\n",
      "2024-10-02 21:34:18.813458: Epoch time: 131.65 s\n",
      "2024-10-02 21:34:18.813572: Yayy! New best EMA pseudo Dice: 0.9515\n",
      "2024-10-02 21:34:20.374914: \n",
      "2024-10-02 21:34:20.375057: Epoch 34\n",
      "2024-10-02 21:34:20.375181: Current learning rate: 0.00969\n",
      "2024-10-02 21:36:26.630544: train_loss -0.9259\n",
      "2024-10-02 21:36:26.630749: val_loss -0.936\n",
      "2024-10-02 21:36:26.630830: Pseudo dice [0.9544, 0.975]\n",
      "2024-10-02 21:36:26.630894: Epoch time: 126.26 s\n",
      "2024-10-02 21:36:26.630941: Yayy! New best EMA pseudo Dice: 0.9528\n",
      "2024-10-02 21:36:28.098009: \n",
      "2024-10-02 21:36:28.098130: Epoch 35\n",
      "2024-10-02 21:36:28.098217: Current learning rate: 0.00968\n",
      "2024-10-02 21:38:37.467602: train_loss -0.9255\n",
      "2024-10-02 21:38:37.467911: val_loss -0.9365\n",
      "2024-10-02 21:38:37.468519: Pseudo dice [0.9545, 0.975]\n",
      "2024-10-02 21:38:37.468708: Epoch time: 129.37 s\n",
      "2024-10-02 21:38:37.468867: Yayy! New best EMA pseudo Dice: 0.954\n",
      "2024-10-02 21:38:40.462212: \n",
      "2024-10-02 21:38:40.462398: Epoch 36\n",
      "2024-10-02 21:38:40.462544: Current learning rate: 0.00968\n",
      "2024-10-02 21:40:52.367472: train_loss -0.9261\n",
      "2024-10-02 21:40:52.367683: val_loss -0.9375\n",
      "2024-10-02 21:40:52.367793: Pseudo dice [0.9556, 0.9753]\n",
      "2024-10-02 21:40:52.367883: Epoch time: 131.91 s\n",
      "2024-10-02 21:40:52.368230: Yayy! New best EMA pseudo Dice: 0.9551\n",
      "2024-10-02 21:40:53.807244: \n",
      "2024-10-02 21:40:53.807685: Epoch 37\n",
      "2024-10-02 21:40:53.807854: Current learning rate: 0.00967\n",
      "2024-10-02 21:43:15.207069: train_loss -0.9269\n",
      "2024-10-02 21:43:15.207302: val_loss -0.937\n",
      "2024-10-02 21:43:15.207415: Pseudo dice [0.9549, 0.9755]\n",
      "2024-10-02 21:43:15.207483: Epoch time: 141.4 s\n",
      "2024-10-02 21:43:15.207535: Yayy! New best EMA pseudo Dice: 0.9561\n",
      "2024-10-02 21:43:16.687604: \n",
      "2024-10-02 21:43:16.687956: Epoch 38\n",
      "2024-10-02 21:43:16.688150: Current learning rate: 0.00966\n",
      "2024-10-02 21:45:22.993262: train_loss -0.9271\n",
      "2024-10-02 21:45:22.993477: val_loss -0.937\n",
      "2024-10-02 21:45:22.993574: Pseudo dice [0.9549, 0.9752]\n",
      "2024-10-02 21:45:22.993823: Epoch time: 126.31 s\n",
      "2024-10-02 21:45:22.993934: Yayy! New best EMA pseudo Dice: 0.957\n",
      "2024-10-02 21:45:24.315989: \n",
      "2024-10-02 21:45:24.316327: Epoch 39\n",
      "2024-10-02 21:45:24.316458: Current learning rate: 0.00965\n",
      "2024-10-02 21:47:28.804749: train_loss -0.9281\n",
      "2024-10-02 21:47:28.804951: val_loss -0.9382\n",
      "2024-10-02 21:47:28.805017: Pseudo dice [0.9561, 0.9756]\n",
      "2024-10-02 21:47:28.805078: Epoch time: 124.49 s\n",
      "2024-10-02 21:47:28.805336: Yayy! New best EMA pseudo Dice: 0.9579\n",
      "2024-10-02 21:47:30.192645: \n",
      "2024-10-02 21:47:30.193224: Epoch 40\n",
      "2024-10-02 21:47:30.193409: Current learning rate: 0.00964\n",
      "2024-10-02 21:49:42.039378: train_loss -0.9279\n",
      "2024-10-02 21:49:42.039777: val_loss -0.9383\n",
      "2024-10-02 21:49:42.040017: Pseudo dice [0.9558, 0.9758]\n",
      "2024-10-02 21:49:42.040128: Epoch time: 131.85 s\n",
      "2024-10-02 21:49:42.040199: Yayy! New best EMA pseudo Dice: 0.9587\n",
      "2024-10-02 21:49:43.379732: \n",
      "2024-10-02 21:49:43.380057: Epoch 41\n",
      "2024-10-02 21:49:43.380204: Current learning rate: 0.00963\n",
      "2024-10-02 21:51:47.773107: train_loss -0.9286\n",
      "2024-10-02 21:51:47.773354: val_loss -0.9384\n",
      "2024-10-02 21:51:47.773449: Pseudo dice [0.9559, 0.9759]\n",
      "2024-10-02 21:51:47.773518: Epoch time: 124.39 s\n",
      "2024-10-02 21:51:47.773569: Yayy! New best EMA pseudo Dice: 0.9594\n",
      "2024-10-02 21:51:48.964635: \n",
      "2024-10-02 21:51:48.964822: Epoch 42\n",
      "2024-10-02 21:51:48.964940: Current learning rate: 0.00962\n",
      "2024-10-02 21:54:04.485044: train_loss -0.9286\n",
      "2024-10-02 21:54:04.485210: val_loss -0.9387\n",
      "2024-10-02 21:54:04.485272: Pseudo dice [0.9559, 0.9762]\n",
      "2024-10-02 21:54:04.485352: Epoch time: 135.52 s\n",
      "2024-10-02 21:54:04.485404: Yayy! New best EMA pseudo Dice: 0.9601\n",
      "2024-10-02 21:54:05.709038: \n",
      "2024-10-02 21:54:05.709179: Epoch 43\n",
      "2024-10-02 21:54:05.709332: Current learning rate: 0.00961\n",
      "2024-10-02 21:56:19.560583: train_loss -0.9293\n",
      "2024-10-02 21:56:19.560866: val_loss -0.9395\n",
      "2024-10-02 21:56:19.560988: Pseudo dice [0.9567, 0.9766]\n",
      "2024-10-02 21:56:19.561102: Epoch time: 133.85 s\n",
      "2024-10-02 21:56:19.561178: Yayy! New best EMA pseudo Dice: 0.9607\n",
      "2024-10-02 21:56:20.817219: \n",
      "2024-10-02 21:56:20.817359: Epoch 44\n",
      "2024-10-02 21:56:20.817461: Current learning rate: 0.0096\n",
      "2024-10-02 21:58:33.527288: train_loss -0.929\n",
      "2024-10-02 21:58:33.527561: val_loss -0.9396\n",
      "2024-10-02 21:58:33.527715: Pseudo dice [0.9566, 0.9766]\n",
      "2024-10-02 21:58:33.527888: Epoch time: 132.71 s\n",
      "2024-10-02 21:58:33.527968: Yayy! New best EMA pseudo Dice: 0.9613\n",
      "2024-10-02 21:58:34.686424: \n",
      "2024-10-02 21:58:34.686555: Epoch 45\n",
      "2024-10-02 21:58:34.686656: Current learning rate: 0.00959\n",
      "2024-10-02 22:00:49.461674: train_loss -0.9297\n",
      "2024-10-02 22:00:49.461870: val_loss -0.9406\n",
      "2024-10-02 22:00:49.461952: Pseudo dice [0.9575, 0.9768]\n",
      "2024-10-02 22:00:49.462036: Epoch time: 134.78 s\n",
      "2024-10-02 22:00:49.462108: Yayy! New best EMA pseudo Dice: 0.9619\n",
      "2024-10-02 22:00:50.726146: \n",
      "2024-10-02 22:00:50.726571: Epoch 46\n",
      "2024-10-02 22:00:50.726673: Current learning rate: 0.00959\n",
      "2024-10-02 22:03:03.129603: train_loss -0.9298\n",
      "2024-10-02 22:03:03.129878: val_loss -0.9405\n",
      "2024-10-02 22:03:03.129991: Pseudo dice [0.9575, 0.9767]\n",
      "2024-10-02 22:03:03.130098: Epoch time: 132.4 s\n",
      "2024-10-02 22:03:03.130193: Yayy! New best EMA pseudo Dice: 0.9624\n",
      "2024-10-02 22:03:04.365382: \n",
      "2024-10-02 22:03:04.365671: Epoch 47\n",
      "2024-10-02 22:03:04.365845: Current learning rate: 0.00958\n",
      "2024-10-02 22:05:17.757854: train_loss -0.9304\n",
      "2024-10-02 22:05:17.758037: val_loss -0.9408\n",
      "2024-10-02 22:05:17.758105: Pseudo dice [0.9574, 0.9769]\n",
      "2024-10-02 22:05:17.758169: Epoch time: 133.39 s\n",
      "2024-10-02 22:05:17.758245: Yayy! New best EMA pseudo Dice: 0.9629\n",
      "2024-10-02 22:05:19.117713: \n",
      "2024-10-02 22:05:19.117847: Epoch 48\n",
      "2024-10-02 22:05:19.117966: Current learning rate: 0.00957\n",
      "2024-10-02 22:07:50.845338: train_loss -0.9306\n",
      "2024-10-02 22:07:50.845518: val_loss -0.9419\n",
      "2024-10-02 22:07:50.846111: Pseudo dice [0.9582, 0.9771]\n",
      "2024-10-02 22:07:50.846314: Epoch time: 151.73 s\n",
      "2024-10-02 22:07:50.846529: Yayy! New best EMA pseudo Dice: 0.9634\n",
      "2024-10-02 22:07:52.090519: \n",
      "2024-10-02 22:07:52.090854: Epoch 49\n",
      "2024-10-02 22:07:52.090974: Current learning rate: 0.00956\n",
      "2024-10-02 22:10:11.036645: train_loss -0.9309\n",
      "2024-10-02 22:10:11.036848: val_loss -0.9417\n",
      "2024-10-02 22:10:11.036914: Pseudo dice [0.9583, 0.9774]\n",
      "2024-10-02 22:10:11.036977: Epoch time: 138.95 s\n",
      "2024-10-02 22:10:11.271744: Yayy! New best EMA pseudo Dice: 0.9638\n",
      "2024-10-02 22:10:12.481402: \n",
      "2024-10-02 22:10:12.481533: Epoch 50\n",
      "2024-10-02 22:10:12.481660: Current learning rate: 0.00955\n",
      "2024-10-02 22:12:21.230674: train_loss -0.9309\n",
      "2024-10-02 22:12:21.231034: val_loss -0.94\n",
      "2024-10-02 22:12:21.231412: Pseudo dice [0.9569, 0.9765]\n",
      "2024-10-02 22:12:21.231565: Epoch time: 128.75 s\n",
      "2024-10-02 22:12:21.231653: Yayy! New best EMA pseudo Dice: 0.9641\n",
      "2024-10-02 22:12:22.428150: \n",
      "2024-10-02 22:12:22.428488: Epoch 51\n",
      "2024-10-02 22:12:22.428622: Current learning rate: 0.00954\n",
      "2024-10-02 22:14:30.336500: train_loss -0.9314\n",
      "2024-10-02 22:14:30.336771: val_loss -0.9413\n",
      "2024-10-02 22:14:30.336872: Pseudo dice [0.958, 0.9772]\n",
      "2024-10-02 22:14:30.336965: Epoch time: 127.91 s\n",
      "2024-10-02 22:14:30.337038: Yayy! New best EMA pseudo Dice: 0.9645\n",
      "2024-10-02 22:14:31.562925: \n",
      "2024-10-02 22:14:31.563102: Epoch 52\n",
      "2024-10-02 22:14:31.563238: Current learning rate: 0.00953\n",
      "2024-10-02 22:16:52.451707: train_loss -0.9316\n",
      "2024-10-02 22:16:52.451996: val_loss -0.9423\n",
      "2024-10-02 22:16:52.452297: Pseudo dice [0.9589, 0.9773]\n",
      "2024-10-02 22:16:52.452411: Epoch time: 140.89 s\n",
      "2024-10-02 22:16:52.452474: Yayy! New best EMA pseudo Dice: 0.9648\n",
      "2024-10-02 22:16:54.002451: \n",
      "2024-10-02 22:16:54.002851: Epoch 53\n",
      "2024-10-02 22:16:54.002985: Current learning rate: 0.00952\n",
      "2024-10-02 22:19:02.721338: train_loss -0.932\n",
      "2024-10-02 22:19:02.721762: val_loss -0.9422\n",
      "2024-10-02 22:19:02.721927: Pseudo dice [0.9586, 0.9775]\n",
      "2024-10-02 22:19:02.722295: Epoch time: 128.72 s\n",
      "2024-10-02 22:19:02.722407: Yayy! New best EMA pseudo Dice: 0.9652\n",
      "2024-10-02 22:19:05.494449: \n",
      "2024-10-02 22:19:05.494834: Epoch 54\n",
      "2024-10-02 22:19:05.494969: Current learning rate: 0.00951\n",
      "2024-10-02 22:21:30.282663: train_loss -0.9323\n",
      "2024-10-02 22:21:30.282893: val_loss -0.9431\n",
      "2024-10-02 22:21:30.283003: Pseudo dice [0.9594, 0.9778]\n",
      "2024-10-02 22:21:30.283103: Epoch time: 144.79 s\n",
      "2024-10-02 22:21:30.283177: Yayy! New best EMA pseudo Dice: 0.9655\n",
      "2024-10-02 22:21:31.606670: \n",
      "2024-10-02 22:21:31.606850: Epoch 55\n",
      "2024-10-02 22:21:31.606995: Current learning rate: 0.0095\n",
      "2024-10-02 22:23:48.667918: train_loss -0.9322\n",
      "2024-10-02 22:23:48.668231: val_loss -0.9419\n",
      "2024-10-02 22:23:48.668375: Pseudo dice [0.9581, 0.9776]\n",
      "2024-10-02 22:23:48.668983: Epoch time: 137.06 s\n",
      "2024-10-02 22:23:48.669088: Yayy! New best EMA pseudo Dice: 0.9657\n",
      "2024-10-02 22:23:49.982212: \n",
      "2024-10-02 22:23:49.982478: Epoch 56\n",
      "2024-10-02 22:23:49.982655: Current learning rate: 0.00949\n",
      "2024-10-02 22:26:05.196109: train_loss -0.9329\n",
      "2024-10-02 22:26:05.196561: val_loss -0.9426\n",
      "2024-10-02 22:26:05.196637: Pseudo dice [0.9588, 0.9775]\n",
      "2024-10-02 22:26:05.196709: Epoch time: 135.21 s\n",
      "2024-10-02 22:26:05.196754: Yayy! New best EMA pseudo Dice: 0.966\n",
      "2024-10-02 22:26:06.498107: \n",
      "2024-10-02 22:26:06.498247: Epoch 57\n",
      "2024-10-02 22:26:06.498356: Current learning rate: 0.00949\n",
      "2024-10-02 22:28:18.195619: train_loss -0.933\n",
      "2024-10-02 22:28:18.195801: val_loss -0.9431\n",
      "2024-10-02 22:28:18.195881: Pseudo dice [0.9591, 0.9778]\n",
      "2024-10-02 22:28:18.196015: Epoch time: 131.7 s\n",
      "2024-10-02 22:28:18.196172: Yayy! New best EMA pseudo Dice: 0.9662\n",
      "2024-10-02 22:28:19.484172: \n",
      "2024-10-02 22:28:19.484370: Epoch 58\n",
      "2024-10-02 22:28:19.484566: Current learning rate: 0.00948\n",
      "2024-10-02 22:30:29.397131: train_loss -0.933\n",
      "2024-10-02 22:30:29.397631: val_loss -0.9433\n",
      "2024-10-02 22:30:29.397836: Pseudo dice [0.9596, 0.9777]\n",
      "2024-10-02 22:30:29.397957: Epoch time: 129.91 s\n",
      "2024-10-02 22:30:29.398196: Yayy! New best EMA pseudo Dice: 0.9665\n",
      "2024-10-02 22:30:30.786657: \n",
      "2024-10-02 22:30:30.787037: Epoch 59\n",
      "2024-10-02 22:30:30.787238: Current learning rate: 0.00947\n",
      "2024-10-02 22:33:19.066346: train_loss -0.9333\n",
      "2024-10-02 22:33:19.066554: val_loss -0.9434\n",
      "2024-10-02 22:33:19.066641: Pseudo dice [0.9593, 0.9781]\n",
      "2024-10-02 22:33:19.066721: Epoch time: 168.28 s\n",
      "2024-10-02 22:33:19.066786: Yayy! New best EMA pseudo Dice: 0.9667\n",
      "2024-10-02 22:33:20.520719: \n",
      "2024-10-02 22:33:20.521172: Epoch 60\n",
      "2024-10-02 22:33:20.521318: Current learning rate: 0.00946\n",
      "2024-10-02 22:35:52.540821: train_loss -0.9339\n",
      "2024-10-02 22:35:52.541030: val_loss -0.9436\n",
      "2024-10-02 22:35:52.541153: Pseudo dice [0.9597, 0.9778]\n",
      "2024-10-02 22:35:52.541279: Epoch time: 152.02 s\n",
      "2024-10-02 22:35:52.541378: Yayy! New best EMA pseudo Dice: 0.9669\n",
      "2024-10-02 22:35:53.858548: \n",
      "2024-10-02 22:35:53.858736: Epoch 61\n",
      "2024-10-02 22:35:53.858912: Current learning rate: 0.00945\n",
      "2024-10-02 22:38:08.557283: train_loss -0.9333\n",
      "2024-10-02 22:38:08.557671: val_loss -0.9441\n",
      "2024-10-02 22:38:08.557790: Pseudo dice [0.9599, 0.9782]\n",
      "2024-10-02 22:38:08.557883: Epoch time: 134.7 s\n",
      "2024-10-02 22:38:08.558102: Yayy! New best EMA pseudo Dice: 0.9671\n",
      "2024-10-02 22:38:09.964667: \n",
      "2024-10-02 22:38:09.964951: Epoch 62\n",
      "2024-10-02 22:38:09.965131: Current learning rate: 0.00944\n",
      "2024-10-02 22:40:22.208296: train_loss -0.9334\n",
      "2024-10-02 22:40:22.208756: val_loss -0.9431\n",
      "2024-10-02 22:40:22.209049: Pseudo dice [0.9591, 0.9777]\n",
      "2024-10-02 22:40:22.209252: Epoch time: 132.24 s\n",
      "2024-10-02 22:40:22.209368: Yayy! New best EMA pseudo Dice: 0.9672\n",
      "2024-10-02 22:40:23.526947: \n",
      "2024-10-02 22:40:23.527073: Epoch 63\n",
      "2024-10-02 22:40:23.527168: Current learning rate: 0.00943\n",
      "2024-10-02 22:42:45.087033: train_loss -0.9337\n",
      "2024-10-02 22:42:45.087427: val_loss -0.9436\n",
      "2024-10-02 22:42:45.087524: Pseudo dice [0.9595, 0.9782]\n",
      "2024-10-02 22:42:45.087632: Epoch time: 141.56 s\n",
      "2024-10-02 22:42:45.087734: Yayy! New best EMA pseudo Dice: 0.9674\n",
      "2024-10-02 22:42:46.349272: \n",
      "2024-10-02 22:42:46.349478: Epoch 64\n",
      "2024-10-02 22:42:46.349585: Current learning rate: 0.00942\n",
      "2024-10-02 22:45:03.787454: train_loss -0.9343\n",
      "2024-10-02 22:45:03.787811: val_loss -0.9445\n",
      "2024-10-02 22:45:03.787932: Pseudo dice [0.9597, 0.9786]\n",
      "2024-10-02 22:45:03.788127: Epoch time: 137.44 s\n",
      "2024-10-02 22:45:03.788204: Yayy! New best EMA pseudo Dice: 0.9676\n",
      "2024-10-02 22:45:05.121408: \n",
      "2024-10-02 22:45:05.121560: Epoch 65\n",
      "2024-10-02 22:45:05.121674: Current learning rate: 0.00941\n",
      "2024-10-02 22:47:36.716253: train_loss -0.935\n",
      "2024-10-02 22:47:36.716713: val_loss -0.9449\n",
      "2024-10-02 22:47:36.716807: Pseudo dice [0.9604, 0.9785]\n",
      "2024-10-02 22:47:36.716877: Epoch time: 151.6 s\n",
      "2024-10-02 22:47:36.716938: Yayy! New best EMA pseudo Dice: 0.9678\n",
      "2024-10-02 22:47:37.929284: \n",
      "2024-10-02 22:47:37.929425: Epoch 66\n",
      "2024-10-02 22:47:37.929821: Current learning rate: 0.0094\n",
      "2024-10-02 22:50:17.738438: train_loss -0.9356\n",
      "2024-10-02 22:50:17.738667: val_loss -0.9454\n",
      "2024-10-02 22:50:17.738755: Pseudo dice [0.9609, 0.979]\n",
      "2024-10-02 22:50:17.738967: Epoch time: 159.81 s\n",
      "2024-10-02 22:50:17.739075: Yayy! New best EMA pseudo Dice: 0.968\n",
      "2024-10-02 22:50:19.041770: \n",
      "2024-10-02 22:50:19.041902: Epoch 67\n",
      "2024-10-02 22:50:19.041990: Current learning rate: 0.00939\n",
      "2024-10-02 22:52:36.248130: train_loss -0.9361\n",
      "2024-10-02 22:52:36.248304: val_loss -0.9459\n",
      "2024-10-02 22:52:36.248393: Pseudo dice [0.9613, 0.979]\n",
      "2024-10-02 22:52:36.248466: Epoch time: 137.21 s\n",
      "2024-10-02 22:52:36.248526: Yayy! New best EMA pseudo Dice: 0.9682\n",
      "2024-10-02 22:52:37.921823: \n",
      "2024-10-02 22:52:37.922334: Epoch 68\n",
      "2024-10-02 22:52:37.922514: Current learning rate: 0.00939\n",
      "2024-10-02 22:54:33.317184: train_loss -0.9357\n",
      "2024-10-02 22:54:33.317400: val_loss -0.9452\n",
      "2024-10-02 22:54:33.317512: Pseudo dice [0.9608, 0.9785]\n",
      "2024-10-02 22:54:33.317606: Epoch time: 115.4 s\n",
      "2024-10-02 22:54:33.317682: Yayy! New best EMA pseudo Dice: 0.9683\n",
      "2024-10-02 22:54:34.501232: \n",
      "2024-10-02 22:54:34.501347: Epoch 69\n",
      "2024-10-02 22:54:34.501434: Current learning rate: 0.00938\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tommy/miniconda3/envs/next-synthseg/bin/nnUNetv2_train\", line 8, in <module>\n",
      "    sys.exit(run_training_entry())\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/nnunetv2/run/run_training.py\", line 275, in run_training_entry\n",
      "    run_training(args.dataset_name_or_id, args.configuration, args.fold, args.tr, args.p, args.pretrained_weights,\n",
      "  File \"/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/nnunetv2/run/run_training.py\", line 211, in run_training\n",
      "    nnunet_trainer.run_training()\n",
      "  File \"/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 1370, in run_training\n",
      "    train_outputs.append(self.train_step(next(self.dataloader_train)))\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/nnunetv2/training/nnUNetTrainer/nnUNetTrainer.py\", line 1002, in train_step\n",
      "    self.grad_scaler.step(self.optimizer)\n",
      "  File \"/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/torch/amp/grad_scaler.py\", line 454, in step\n",
      "    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/torch/amp/grad_scaler.py\", line 351, in _maybe_opt_step\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/torch/amp/grad_scaler.py\", line 351, in <genexpr>\n",
      "    if not sum(v.item() for v in optimizer_state[\"found_inf_per_device\"].values()):\n",
      "               ^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!nnUNet_n_proc_DA=8 nnUNetv2_train 1 2d all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nnUNetv2_find_best_configuration [-h] [-p P [P ...]] [-c C [C ...]]\n",
      "                                        [-tr TR [TR ...]] [-np NP]\n",
      "                                        [-f F [F ...]] [--disable_ensembling]\n",
      "                                        [--no_overwrite]\n",
      "                                        dataset_name_or_id\n",
      "nnUNetv2_find_best_configuration: error: argument -f: invalid int value: 'all'\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_find_best_configuration 1 -c 2d -f all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################################################################\n",
      "Please cite the following paper when using nnU-Net:\n",
      "Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), 203-211.\n",
      "#######################################################################\n",
      "\n",
      "/home/tommy/miniconda3/envs/next-synthseg/lib/python3.12/site-packages/nnunetv2/inference/predict_from_raw_data.py:84: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(join(model_training_output_dir, f'fold_{f}', checkpoint_name),\n",
      "There are 5 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 5 cases that I would like to predict\n",
      "\n",
      "Predicting aaa_000:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████| 126/126 [00:03<00:00, 40.05it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with aaa_000\n",
      "\n",
      "Predicting aaa_001:\n",
      "perform_everything_on_device: True\n",
      "100%|███████████████████████████████████████████| 99/99 [00:02<00:00, 49.02it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with aaa_001\n",
      "\n",
      "Predicting aaa_002:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████| 462/462 [00:09<00:00, 51.27it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with aaa_002\n",
      "\n",
      "Predicting aaa_003:\n",
      "perform_everything_on_device: True\n",
      "100%|███████████████████████████████████████| 1464/1464 [00:27<00:00, 52.66it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with aaa_003\n",
      "\n",
      "Predicting aaa_004:\n",
      "perform_everything_on_device: True\n",
      "100%|█████████████████████████████████████████| 560/560 [00:10<00:00, 51.23it/s]\n",
      "sending off prediction to background worker for resampling and export\n",
      "done with aaa_004\n"
     ]
    }
   ],
   "source": [
    "!nnUNetv2_predict -i \"data/nnunet_raw/Dataset001_aaa/imagesTs\" -o \"data/nnunet_results/Dataset001_aaa/labelsTs\" -d 1 -c 2d -f all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cd82ee1ed1d49a28b08f2977658035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='slice_index_0', max=125), IntSlider(value=0, description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = nib.load(\n",
    "    \"data/nnunet_raw/Dataset001_aaa/imagesTs/aaa_000_0000.nii.gz\"\n",
    ").get_fdata()\n",
    "label = nib.load(\n",
    "    \"data/nnunet_results/Dataset001_aaa/labelsTs/aaa_000.nii.gz\"\n",
    ").get_fdata()\n",
    "ground_truth = nib.load(\n",
    "    \"data/nnunet_raw/Dataset001_aaa/labelsTs/aaa_000.nii.gz\"\n",
    ").get_fdata()\n",
    "check_image(\n",
    "    [\n",
    "        {\"image\": label, \"title\": \"label\", \"is_label\": True},\n",
    "        {\"image\": ground_truth, \"title\": \"ground_truth\", \"is_label\": True},\n",
    "        {\"image\": image, \"title\": \"image\", \"is_label\": False},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((110, 104, 126), (110, 104, 126))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape,ground_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice score for class 1: 0.8638\n",
      "Dice score for class 2: 0.8659\n",
      "Mean Dice score: 0.8649\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def calculate_dice_score(pred, target):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    pred = torch.from_numpy(pred).float()\n",
    "    target = torch.from_numpy(target).float()\n",
    "    \n",
    "    # Flatten the tensors\n",
    "    pred = pred.flatten()\n",
    "    target = target.flatten()\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    \n",
    "    # Calculate Dice score\n",
    "    dice = (2. * intersection) / (union + 1e-8)  # Adding small epsilon to avoid division by zero\n",
    "    \n",
    "    return dice.item()\n",
    "\n",
    "# Calculate Dice score for each class\n",
    "num_classes = int(max(label.max(), ground_truth.max())) + 1\n",
    "dice_scores = []\n",
    "\n",
    "for class_id in range(1, num_classes):  # Skipping background class (0)\n",
    "    pred_class = (label == class_id).astype(int)\n",
    "    target_class = (ground_truth == class_id).astype(int)\n",
    "    dice_score = calculate_dice_score(pred_class, target_class)\n",
    "    dice_scores.append(dice_score)\n",
    "    print(f\"Dice score for class {class_id}: {dice_score:.4f}\")\n",
    "\n",
    "# Calculate mean Dice score\n",
    "mean_dice = sum(dice_scores) / len(dice_scores)\n",
    "print(f\"Mean Dice score: {mean_dice:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnUNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
